# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10IgMkGKF3Fzg4iyDWOLJucoEa5Fv_OnM
"""

# Install UCI ML Repo package
!pip install ucimlrepo --quiet

# Import all necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random as r

from ucimlrepo import fetch_ucirepo
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load Wine dataset from UCI
wine_data = fetch_ucirepo(id=186)

# Features and targets
X_all = wine_data.data.features
y_all = wine_data.data.targets.squeeze().astype(int)  # Convert to int

# Define parameter space and setup logs
kernelList = ['linear', 'poly', 'rbf', 'sigmoid']
overall_results = []
best_overall_accuracy = 0
best_sample_log = None

# For reproducibility
r.seed(42)

import time

# Use only stable kernels first â€” add back others once stable
kernelList = ['linear', 'rbf']
overall_results = []
best_overall_accuracy = 0
best_sample_log = None

for sample_no in range(10):
    print(f"\nðŸ” Processing Sample {sample_no}...")

    # Split data into train/test sets
    X_train_full, X_test, y_train_full, y_test = train_test_split(
        X_all, y_all, test_size=0.3, random_state=sample_no)

    # Select only 50 training rows
    X_train = X_train_full.sample(n=50, random_state=sample_no)
    y_train = y_train_full.loc[X_train.index]

    X_train = X_train.reset_index(drop=True)
    y_train = y_train.reset_index(drop=True)

    # Track best performance for this sample
    bestAccuracy = 0
    bestKernel = ""
    bestC = 0
    bestGamma = 0
    iteration_log = []

    for i in range(100):
        kernel = r.choice(kernelList)
        C = round(r.uniform(0.5, 5.0), 2)         # safer range
        gamma = round(r.uniform(0.01, 0.5), 3)    # avoid extremes

        try:
            model = SVC(kernel=kernel, C=C, gamma=gamma)

            # â±ï¸ Time training
            start = time.time()
            model.fit(X_train, y_train)
            fit_time = time.time() - start

            # Skip if taking too long
            if fit_time > 3:
                print(f"â³ Skipped slow model at iteration {i} (took {fit_time:.2f}s)")
                continue

            preds = model.predict(X_test)
            acc = accuracy_score(y_test, preds) * 100

        except Exception as e:
            print(f"âš ï¸ Error at iteration {i}: {e}")
            acc = 0

        if acc > bestAccuracy:
            bestAccuracy = acc
            bestKernel = kernel
            bestC = C
            bestGamma = gamma

        if i % 10 == 0:
            iteration_log.append({
                'Iteration': i,
                'Fitness (bestAccuracy)': round(bestAccuracy, 4),
                'bestKernel': bestKernel,
                'bestC': bestC,
                'bestGamma': bestGamma,
                'sample_no': sample_no
            })

    # Store results for this sample
    overall_results.append({
        'Sample': sample_no,
        'BestAccuracy': round(bestAccuracy, 4),
        'Kernel': bestKernel,
        'C': bestC,
        'Gamma': bestGamma
    })

    # Track best overall sample
    if bestAccuracy > best_overall_accuracy:
        best_overall_accuracy = bestAccuracy
        best_sample_log = pd.DataFrame(iteration_log)

# Plot convergence graph for best sample
plt.figure(figsize=(10, 5))
plt.plot(best_sample_log['Iteration'], best_sample_log['Fitness (bestAccuracy)'], marker='o', linestyle='-', color='darkorange')
plt.title('Convergence Plot for Best Sample (50-row Training)', fontsize=14)
plt.xlabel('Iteration')
plt.ylabel('Best Accuracy (%)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Optional: Save overall results to CSV
results_df = pd.DataFrame(overall_results)
results_df.to_csv("svm_optimization_results.csv", index=False)

# Show top results
results_df.sort_values(by='BestAccuracy', ascending=False).head()